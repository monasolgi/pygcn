{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import __future__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "   #onehot vector for 7 classes in output\n",
    "\n",
    "def load_data(path=\"\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),dtype=np.dtype(str))\n",
    "    \n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    #csr=Compressed Sparse Row \n",
    "    #array  of rows\n",
    "    \n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "    #last col  of idx_features_labels=7 classes\n",
    "    \n",
    "    # build graph\n",
    "    \n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    #first col in every row=doc id\n",
    "    \n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    #{'31336': 0, '1061127':1, ...}\n",
    "    \n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    \n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "    #each doc is connected to which doc (citation) \n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    #idx_train = range(2)\n",
    "    #idx_val = range(2, 3)\n",
    "    #idx_test = range(4)\n",
    "    \n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 501)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    #sum in every row \n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    # every sum to the power of -1 \n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    #diagonal matrice \n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "#idx_features_labels= load_data()\n",
    "#idx_features_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels= load_data()\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features= load_data()\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx=load_data()\n",
    "#idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx_map=load_data()\n",
    "#idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edges_unordered=load_data()\n",
    "#edges_unordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edges=load_data()\n",
    "#edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj=load_data()\n",
    "#adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "#class parameter ,param haro cache mikone\n",
    "\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        \n",
    "        #Sparse matrix multiplication=spmm\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ') '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers import GraphConvolution\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid,nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        #self.gc2 = GraphConvolution(nhid, nhid2)\n",
    "        #self.gc3 = GraphConvolution(nhid2, nclass)\n",
    "        \n",
    "        #adding a 3th layer didn't outperform\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        #can also use tanh,not very different result\n",
    "        \n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "        #print(x.size())\n",
    "    \n",
    "        x = self.gc2(x, adj)\n",
    "        \n",
    "        #x = self.gc3(x, adj)\n",
    "        #return x\n",
    "       # print(x.size())\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Epoch: 0001 loss_train: 1.9318 acc_train: 0.2143 loss_val: 1.9087 acc_val: 0.3500 time: 0.0899s\n",
      "Epoch: 0002 loss_train: 1.9290 acc_train: 0.2929 loss_val: 1.8981 acc_val: 0.3500 time: 0.0819s\n",
      "Epoch: 0003 loss_train: 1.9186 acc_train: 0.2929 loss_val: 1.8884 acc_val: 0.3500 time: 0.0770s\n",
      "Epoch: 0004 loss_train: 1.9057 acc_train: 0.2929 loss_val: 1.8791 acc_val: 0.3500 time: 0.0859s\n",
      "Epoch: 0005 loss_train: 1.8902 acc_train: 0.2929 loss_val: 1.8702 acc_val: 0.3500 time: 0.0849s\n",
      "Epoch: 0006 loss_train: 1.8876 acc_train: 0.2929 loss_val: 1.8615 acc_val: 0.3500 time: 0.0820s\n",
      "Epoch: 0007 loss_train: 1.8865 acc_train: 0.2929 loss_val: 1.8531 acc_val: 0.3500 time: 0.1019s\n",
      "Epoch: 0008 loss_train: 1.8740 acc_train: 0.2929 loss_val: 1.8449 acc_val: 0.3500 time: 0.0790s\n",
      "Epoch: 0009 loss_train: 1.8580 acc_train: 0.2929 loss_val: 1.8368 acc_val: 0.3500 time: 0.0829s\n",
      "Epoch: 0010 loss_train: 1.8539 acc_train: 0.2929 loss_val: 1.8288 acc_val: 0.3500 time: 0.0820s\n",
      "Epoch: 0011 loss_train: 1.8467 acc_train: 0.2929 loss_val: 1.8211 acc_val: 0.3500 time: 0.0839s\n",
      "Epoch: 0012 loss_train: 1.8278 acc_train: 0.2929 loss_val: 1.8136 acc_val: 0.3500 time: 0.0919s\n",
      "Epoch: 0013 loss_train: 1.8355 acc_train: 0.2929 loss_val: 1.8064 acc_val: 0.3500 time: 0.0899s\n",
      "Epoch: 0014 loss_train: 1.8070 acc_train: 0.2929 loss_val: 1.7993 acc_val: 0.3500 time: 0.0790s\n",
      "Epoch: 0015 loss_train: 1.8202 acc_train: 0.2929 loss_val: 1.7925 acc_val: 0.3500 time: 0.0800s\n",
      "Epoch: 0016 loss_train: 1.7954 acc_train: 0.2929 loss_val: 1.7859 acc_val: 0.3500 time: 0.0859s\n",
      "Epoch: 0017 loss_train: 1.7947 acc_train: 0.2929 loss_val: 1.7794 acc_val: 0.3500 time: 0.0839s\n",
      "Epoch: 0018 loss_train: 1.7891 acc_train: 0.2929 loss_val: 1.7731 acc_val: 0.3500 time: 0.0780s\n",
      "Epoch: 0019 loss_train: 1.7764 acc_train: 0.2929 loss_val: 1.7670 acc_val: 0.3500 time: 0.0750s\n",
      "Epoch: 0020 loss_train: 1.7549 acc_train: 0.3000 loss_val: 1.7610 acc_val: 0.3500 time: 0.0809s\n",
      "Epoch: 0021 loss_train: 1.7705 acc_train: 0.3000 loss_val: 1.7554 acc_val: 0.3500 time: 0.0770s\n",
      "Epoch: 0022 loss_train: 1.7772 acc_train: 0.2929 loss_val: 1.7498 acc_val: 0.3500 time: 0.0625s\n",
      "Epoch: 0023 loss_train: 1.7545 acc_train: 0.3000 loss_val: 1.7441 acc_val: 0.3500 time: 0.0781s\n",
      "Epoch: 0024 loss_train: 1.7313 acc_train: 0.3071 loss_val: 1.7384 acc_val: 0.3500 time: 0.0721s\n",
      "Epoch: 0025 loss_train: 1.7552 acc_train: 0.3143 loss_val: 1.7326 acc_val: 0.3500 time: 0.0781s\n",
      "Epoch: 0026 loss_train: 1.6983 acc_train: 0.3214 loss_val: 1.7267 acc_val: 0.3500 time: 0.0835s\n",
      "Epoch: 0027 loss_train: 1.7275 acc_train: 0.3286 loss_val: 1.7204 acc_val: 0.3500 time: 0.0770s\n",
      "Epoch: 0028 loss_train: 1.7088 acc_train: 0.3214 loss_val: 1.7140 acc_val: 0.3500 time: 0.0879s\n",
      "Epoch: 0029 loss_train: 1.6947 acc_train: 0.3143 loss_val: 1.7073 acc_val: 0.3500 time: 0.0849s\n",
      "Epoch: 0030 loss_train: 1.6807 acc_train: 0.3286 loss_val: 1.7004 acc_val: 0.3500 time: 0.0820s\n",
      "Epoch: 0031 loss_train: 1.6761 acc_train: 0.3214 loss_val: 1.6931 acc_val: 0.3500 time: 0.0819s\n",
      "Epoch: 0032 loss_train: 1.6466 acc_train: 0.3429 loss_val: 1.6856 acc_val: 0.3500 time: 0.0819s\n",
      "Epoch: 0033 loss_train: 1.6515 acc_train: 0.3214 loss_val: 1.6777 acc_val: 0.3533 time: 0.0796s\n",
      "Epoch: 0034 loss_train: 1.6323 acc_train: 0.3571 loss_val: 1.6692 acc_val: 0.3500 time: 0.0819s\n",
      "Epoch: 0035 loss_train: 1.6219 acc_train: 0.3214 loss_val: 1.6604 acc_val: 0.3567 time: 0.0810s\n",
      "Epoch: 0036 loss_train: 1.6067 acc_train: 0.3929 loss_val: 1.6512 acc_val: 0.3600 time: 0.0800s\n",
      "Epoch: 0037 loss_train: 1.5844 acc_train: 0.3857 loss_val: 1.6417 acc_val: 0.3600 time: 0.0929s\n",
      "Epoch: 0038 loss_train: 1.5856 acc_train: 0.4071 loss_val: 1.6316 acc_val: 0.3600 time: 0.0849s\n",
      "Epoch: 0039 loss_train: 1.5405 acc_train: 0.4286 loss_val: 1.6213 acc_val: 0.3633 time: 0.0839s\n",
      "Epoch: 0040 loss_train: 1.5517 acc_train: 0.4357 loss_val: 1.6105 acc_val: 0.3833 time: 0.0849s\n",
      "Epoch: 0041 loss_train: 1.5713 acc_train: 0.4214 loss_val: 1.5992 acc_val: 0.3867 time: 0.0879s\n",
      "Epoch: 0042 loss_train: 1.5382 acc_train: 0.4714 loss_val: 1.5873 acc_val: 0.3967 time: 0.0839s\n",
      "Epoch: 0043 loss_train: 1.5128 acc_train: 0.4286 loss_val: 1.5751 acc_val: 0.4167 time: 0.0790s\n",
      "Epoch: 0044 loss_train: 1.4868 acc_train: 0.4643 loss_val: 1.5621 acc_val: 0.4333 time: 0.0839s\n",
      "Epoch: 0045 loss_train: 1.4856 acc_train: 0.4857 loss_val: 1.5489 acc_val: 0.4467 time: 0.0869s\n",
      "Epoch: 0046 loss_train: 1.4699 acc_train: 0.4714 loss_val: 1.5353 acc_val: 0.4533 time: 0.0859s\n",
      "Epoch: 0047 loss_train: 1.4491 acc_train: 0.5143 loss_val: 1.5212 acc_val: 0.4567 time: 0.0829s\n",
      "Epoch: 0048 loss_train: 1.4623 acc_train: 0.4500 loss_val: 1.5070 acc_val: 0.4633 time: 0.0770s\n",
      "Epoch: 0049 loss_train: 1.3996 acc_train: 0.5571 loss_val: 1.4924 acc_val: 0.4633 time: 0.0790s\n",
      "Epoch: 0050 loss_train: 1.4038 acc_train: 0.5071 loss_val: 1.4776 acc_val: 0.4733 time: 0.0800s\n",
      "Epoch: 0051 loss_train: 1.3496 acc_train: 0.5500 loss_val: 1.4628 acc_val: 0.4900 time: 0.0839s\n",
      "Epoch: 0052 loss_train: 1.3503 acc_train: 0.5714 loss_val: 1.4481 acc_val: 0.5000 time: 0.0819s\n",
      "Epoch: 0053 loss_train: 1.3210 acc_train: 0.5786 loss_val: 1.4337 acc_val: 0.5200 time: 0.0800s\n",
      "Epoch: 0054 loss_train: 1.3120 acc_train: 0.6286 loss_val: 1.4191 acc_val: 0.5333 time: 0.0780s\n",
      "Epoch: 0055 loss_train: 1.3111 acc_train: 0.5929 loss_val: 1.4046 acc_val: 0.5433 time: 0.0810s\n",
      "Epoch: 0056 loss_train: 1.2944 acc_train: 0.6571 loss_val: 1.3898 acc_val: 0.5500 time: 0.0819s\n",
      "Epoch: 0057 loss_train: 1.2693 acc_train: 0.6643 loss_val: 1.3751 acc_val: 0.5733 time: 0.0797s\n",
      "Epoch: 0058 loss_train: 1.2553 acc_train: 0.6643 loss_val: 1.3601 acc_val: 0.5900 time: 0.1279s\n",
      "Epoch: 0059 loss_train: 1.2127 acc_train: 0.6786 loss_val: 1.3454 acc_val: 0.6133 time: 0.0989s\n",
      "Epoch: 0060 loss_train: 1.2210 acc_train: 0.6643 loss_val: 1.3305 acc_val: 0.6467 time: 0.0969s\n",
      "Epoch: 0061 loss_train: 1.2145 acc_train: 0.6714 loss_val: 1.3153 acc_val: 0.6700 time: 0.1019s\n",
      "Epoch: 0062 loss_train: 1.1635 acc_train: 0.6857 loss_val: 1.3006 acc_val: 0.6767 time: 0.0929s\n",
      "Epoch: 0063 loss_train: 1.1385 acc_train: 0.7429 loss_val: 1.2858 acc_val: 0.6933 time: 0.0989s\n",
      "Epoch: 0064 loss_train: 1.1271 acc_train: 0.7500 loss_val: 1.2712 acc_val: 0.7067 time: 0.0999s\n",
      "Epoch: 0065 loss_train: 1.1290 acc_train: 0.7214 loss_val: 1.2564 acc_val: 0.7100 time: 0.1309s\n",
      "Epoch: 0066 loss_train: 1.0873 acc_train: 0.7357 loss_val: 1.2421 acc_val: 0.7100 time: 0.0690s\n",
      "Epoch: 0067 loss_train: 1.0546 acc_train: 0.7929 loss_val: 1.2278 acc_val: 0.7167 time: 0.0829s\n",
      "Epoch: 0068 loss_train: 1.0546 acc_train: 0.8000 loss_val: 1.2139 acc_val: 0.7233 time: 0.0829s\n",
      "Epoch: 0069 loss_train: 1.0617 acc_train: 0.7786 loss_val: 1.2003 acc_val: 0.7233 time: 0.0716s\n",
      "Epoch: 0070 loss_train: 1.0017 acc_train: 0.7786 loss_val: 1.1876 acc_val: 0.7267 time: 0.0798s\n",
      "Epoch: 0071 loss_train: 0.9977 acc_train: 0.7571 loss_val: 1.1752 acc_val: 0.7367 time: 0.0781s\n",
      "Epoch: 0072 loss_train: 0.9954 acc_train: 0.7857 loss_val: 1.1621 acc_val: 0.7467 time: 0.0781s\n",
      "Epoch: 0073 loss_train: 1.0027 acc_train: 0.7857 loss_val: 1.1490 acc_val: 0.7567 time: 0.0870s\n",
      "Epoch: 0074 loss_train: 0.9431 acc_train: 0.8143 loss_val: 1.1358 acc_val: 0.7667 time: 0.0849s\n",
      "Epoch: 0075 loss_train: 0.9235 acc_train: 0.7929 loss_val: 1.1240 acc_val: 0.7767 time: 0.0839s\n",
      "Epoch: 0076 loss_train: 0.9584 acc_train: 0.8071 loss_val: 1.1120 acc_val: 0.7800 time: 0.0889s\n",
      "Epoch: 0077 loss_train: 0.9179 acc_train: 0.8286 loss_val: 1.1009 acc_val: 0.7833 time: 0.0720s\n",
      "Epoch: 0078 loss_train: 0.8825 acc_train: 0.8571 loss_val: 1.0900 acc_val: 0.7867 time: 0.0666s\n",
      "Epoch: 0079 loss_train: 0.9166 acc_train: 0.8071 loss_val: 1.0797 acc_val: 0.7933 time: 0.0897s\n",
      "Epoch: 0080 loss_train: 0.8677 acc_train: 0.8286 loss_val: 1.0696 acc_val: 0.7933 time: 0.0839s\n",
      "Epoch: 0081 loss_train: 0.8757 acc_train: 0.8429 loss_val: 1.0597 acc_val: 0.8000 time: 0.0849s\n",
      "Epoch: 0082 loss_train: 0.8613 acc_train: 0.8357 loss_val: 1.0503 acc_val: 0.8000 time: 0.0869s\n",
      "Epoch: 0083 loss_train: 0.8843 acc_train: 0.8429 loss_val: 1.0410 acc_val: 0.8067 time: 0.0800s\n",
      "Epoch: 0084 loss_train: 0.8205 acc_train: 0.8643 loss_val: 1.0318 acc_val: 0.8067 time: 0.0769s\n",
      "Epoch: 0085 loss_train: 0.7860 acc_train: 0.7929 loss_val: 1.0227 acc_val: 0.8067 time: 0.0781s\n",
      "Epoch: 0086 loss_train: 0.8126 acc_train: 0.8143 loss_val: 1.0137 acc_val: 0.8067 time: 0.0861s\n",
      "Epoch: 0087 loss_train: 0.8181 acc_train: 0.8500 loss_val: 1.0046 acc_val: 0.8067 time: 0.0809s\n",
      "Epoch: 0088 loss_train: 0.8039 acc_train: 0.8429 loss_val: 0.9961 acc_val: 0.8033 time: 0.0759s\n",
      "Epoch: 0089 loss_train: 0.7929 acc_train: 0.8286 loss_val: 0.9878 acc_val: 0.8033 time: 0.0820s\n",
      "Epoch: 0090 loss_train: 0.7697 acc_train: 0.8429 loss_val: 0.9810 acc_val: 0.8000 time: 0.0919s\n",
      "Epoch: 0091 loss_train: 0.7873 acc_train: 0.8500 loss_val: 0.9734 acc_val: 0.8000 time: 0.0839s\n",
      "Epoch: 0092 loss_train: 0.7506 acc_train: 0.8571 loss_val: 0.9662 acc_val: 0.7967 time: 0.0839s\n",
      "Epoch: 0093 loss_train: 0.7743 acc_train: 0.8714 loss_val: 0.9574 acc_val: 0.8000 time: 0.0765s\n",
      "Epoch: 0094 loss_train: 0.7625 acc_train: 0.8357 loss_val: 0.9483 acc_val: 0.8133 time: 0.0760s\n",
      "Epoch: 0095 loss_train: 0.7437 acc_train: 0.8786 loss_val: 0.9395 acc_val: 0.8067 time: 0.0809s\n",
      "Epoch: 0096 loss_train: 0.7646 acc_train: 0.8571 loss_val: 0.9321 acc_val: 0.8067 time: 0.0820s\n",
      "Epoch: 0097 loss_train: 0.7321 acc_train: 0.8571 loss_val: 0.9250 acc_val: 0.8067 time: 0.0770s\n",
      "Epoch: 0098 loss_train: 0.7060 acc_train: 0.8500 loss_val: 0.9186 acc_val: 0.8100 time: 0.0810s\n",
      "Epoch: 0099 loss_train: 0.6846 acc_train: 0.8786 loss_val: 0.9134 acc_val: 0.8133 time: 0.0760s\n",
      "Epoch: 0100 loss_train: 0.6906 acc_train: 0.8643 loss_val: 0.9085 acc_val: 0.8133 time: 0.0740s\n",
      "Epoch: 0101 loss_train: 0.6650 acc_train: 0.8643 loss_val: 0.9043 acc_val: 0.7967 time: 0.0841s\n",
      "Epoch: 0102 loss_train: 0.6582 acc_train: 0.8857 loss_val: 0.9004 acc_val: 0.8000 time: 0.0790s\n",
      "Epoch: 0103 loss_train: 0.6311 acc_train: 0.9000 loss_val: 0.8956 acc_val: 0.8000 time: 0.0755s\n",
      "Epoch: 0104 loss_train: 0.6409 acc_train: 0.9000 loss_val: 0.8901 acc_val: 0.8000 time: 0.0779s\n",
      "Epoch: 0105 loss_train: 0.6179 acc_train: 0.9071 loss_val: 0.8847 acc_val: 0.8000 time: 0.0849s\n",
      "Epoch: 0106 loss_train: 0.6674 acc_train: 0.8786 loss_val: 0.8792 acc_val: 0.8033 time: 0.0760s\n",
      "Epoch: 0107 loss_train: 0.6680 acc_train: 0.8929 loss_val: 0.8720 acc_val: 0.8033 time: 0.0770s\n",
      "Epoch: 0108 loss_train: 0.6446 acc_train: 0.8786 loss_val: 0.8650 acc_val: 0.8033 time: 0.0780s\n",
      "Epoch: 0109 loss_train: 0.6335 acc_train: 0.8571 loss_val: 0.8586 acc_val: 0.8133 time: 0.0780s\n",
      "Epoch: 0110 loss_train: 0.6442 acc_train: 0.8429 loss_val: 0.8538 acc_val: 0.8133 time: 0.0760s\n",
      "Epoch: 0111 loss_train: 0.6125 acc_train: 0.9000 loss_val: 0.8491 acc_val: 0.8133 time: 0.0780s\n",
      "Epoch: 0112 loss_train: 0.5929 acc_train: 0.8643 loss_val: 0.8451 acc_val: 0.8133 time: 0.0810s\n",
      "Epoch: 0113 loss_train: 0.6004 acc_train: 0.8714 loss_val: 0.8423 acc_val: 0.8200 time: 0.0770s\n",
      "Epoch: 0114 loss_train: 0.6238 acc_train: 0.8500 loss_val: 0.8404 acc_val: 0.8000 time: 0.0790s\n",
      "Epoch: 0115 loss_train: 0.6281 acc_train: 0.8643 loss_val: 0.8383 acc_val: 0.8033 time: 0.0780s\n",
      "Epoch: 0116 loss_train: 0.6304 acc_train: 0.9071 loss_val: 0.8345 acc_val: 0.8000 time: 0.0772s\n",
      "Epoch: 0117 loss_train: 0.5591 acc_train: 0.8929 loss_val: 0.8312 acc_val: 0.8033 time: 0.0783s\n",
      "Epoch: 0118 loss_train: 0.6411 acc_train: 0.8857 loss_val: 0.8251 acc_val: 0.8033 time: 0.0799s\n",
      "Epoch: 0119 loss_train: 0.6078 acc_train: 0.9000 loss_val: 0.8199 acc_val: 0.8100 time: 0.0693s\n",
      "Epoch: 0120 loss_train: 0.5671 acc_train: 0.8786 loss_val: 0.8145 acc_val: 0.8233 time: 0.0858s\n",
      "Epoch: 0121 loss_train: 0.5343 acc_train: 0.9214 loss_val: 0.8094 acc_val: 0.8267 time: 0.0810s\n",
      "Epoch: 0122 loss_train: 0.5817 acc_train: 0.8643 loss_val: 0.8051 acc_val: 0.8267 time: 0.0740s\n",
      "Epoch: 0123 loss_train: 0.5424 acc_train: 0.9143 loss_val: 0.8015 acc_val: 0.8267 time: 0.0821s\n",
      "Epoch: 0124 loss_train: 0.5852 acc_train: 0.8714 loss_val: 0.7991 acc_val: 0.8233 time: 0.0770s\n",
      "Epoch: 0125 loss_train: 0.5424 acc_train: 0.9214 loss_val: 0.7969 acc_val: 0.8267 time: 0.0739s\n",
      "Epoch: 0126 loss_train: 0.5325 acc_train: 0.9429 loss_val: 0.7963 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0127 loss_train: 0.5556 acc_train: 0.9143 loss_val: 0.7957 acc_val: 0.8033 time: 0.0781s\n",
      "Epoch: 0128 loss_train: 0.5234 acc_train: 0.9143 loss_val: 0.7941 acc_val: 0.8033 time: 0.0781s\n",
      "Epoch: 0129 loss_train: 0.5337 acc_train: 0.8714 loss_val: 0.7918 acc_val: 0.8000 time: 0.0781s\n",
      "Epoch: 0130 loss_train: 0.5425 acc_train: 0.9286 loss_val: 0.7902 acc_val: 0.7967 time: 0.0769s\n",
      "Epoch: 0131 loss_train: 0.5489 acc_train: 0.9071 loss_val: 0.7876 acc_val: 0.7967 time: 0.0781s\n",
      "Epoch: 0132 loss_train: 0.5469 acc_train: 0.9214 loss_val: 0.7832 acc_val: 0.7967 time: 0.0781s\n",
      "Epoch: 0133 loss_train: 0.5354 acc_train: 0.9000 loss_val: 0.7794 acc_val: 0.8000 time: 0.0781s\n",
      "Epoch: 0134 loss_train: 0.5119 acc_train: 0.9143 loss_val: 0.7759 acc_val: 0.8100 time: 0.0781s\n",
      "Epoch: 0135 loss_train: 0.5273 acc_train: 0.9286 loss_val: 0.7725 acc_val: 0.8100 time: 0.0839s\n",
      "Epoch: 0136 loss_train: 0.5060 acc_train: 0.9286 loss_val: 0.7695 acc_val: 0.8267 time: 0.0760s\n",
      "Epoch: 0137 loss_train: 0.5386 acc_train: 0.9429 loss_val: 0.7673 acc_val: 0.8300 time: 0.0800s\n",
      "Epoch: 0138 loss_train: 0.4890 acc_train: 0.9357 loss_val: 0.7656 acc_val: 0.8167 time: 0.0690s\n",
      "Epoch: 0139 loss_train: 0.4737 acc_train: 0.9500 loss_val: 0.7632 acc_val: 0.8133 time: 0.0781s\n",
      "Epoch: 0140 loss_train: 0.5284 acc_train: 0.8929 loss_val: 0.7607 acc_val: 0.8233 time: 0.0781s\n",
      "Epoch: 0141 loss_train: 0.4894 acc_train: 0.9214 loss_val: 0.7593 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0142 loss_train: 0.4729 acc_train: 0.9071 loss_val: 0.7579 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0143 loss_train: 0.4586 acc_train: 0.9357 loss_val: 0.7576 acc_val: 0.8067 time: 0.0781s\n",
      "Epoch: 0144 loss_train: 0.4998 acc_train: 0.9143 loss_val: 0.7563 acc_val: 0.8100 time: 0.0781s\n",
      "Epoch: 0145 loss_train: 0.4824 acc_train: 0.9214 loss_val: 0.7551 acc_val: 0.8100 time: 0.0781s\n",
      "Epoch: 0146 loss_train: 0.4720 acc_train: 0.9000 loss_val: 0.7533 acc_val: 0.8067 time: 0.0781s\n",
      "Epoch: 0147 loss_train: 0.4691 acc_train: 0.9286 loss_val: 0.7509 acc_val: 0.8067 time: 0.0781s\n",
      "Epoch: 0148 loss_train: 0.4611 acc_train: 0.9143 loss_val: 0.7479 acc_val: 0.8100 time: 0.0781s\n",
      "Epoch: 0149 loss_train: 0.4898 acc_train: 0.9071 loss_val: 0.7444 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0150 loss_train: 0.4814 acc_train: 0.9500 loss_val: 0.7405 acc_val: 0.8167 time: 0.0904s\n",
      "Epoch: 0151 loss_train: 0.5321 acc_train: 0.9214 loss_val: 0.7374 acc_val: 0.8200 time: 0.0760s\n",
      "Epoch: 0152 loss_train: 0.4612 acc_train: 0.9214 loss_val: 0.7351 acc_val: 0.8200 time: 0.0797s\n",
      "Epoch: 0153 loss_train: 0.4572 acc_train: 0.9286 loss_val: 0.7332 acc_val: 0.8200 time: 0.0770s\n",
      "Epoch: 0154 loss_train: 0.4469 acc_train: 0.9286 loss_val: 0.7319 acc_val: 0.8200 time: 0.0810s\n",
      "Epoch: 0155 loss_train: 0.4464 acc_train: 0.9357 loss_val: 0.7304 acc_val: 0.8167 time: 0.0800s\n",
      "Epoch: 0156 loss_train: 0.4749 acc_train: 0.9286 loss_val: 0.7297 acc_val: 0.8133 time: 0.0780s\n",
      "Epoch: 0157 loss_train: 0.4566 acc_train: 0.9500 loss_val: 0.7289 acc_val: 0.8167 time: 0.0620s\n",
      "Epoch: 0158 loss_train: 0.4509 acc_train: 0.9357 loss_val: 0.7282 acc_val: 0.8133 time: 0.0781s\n",
      "Epoch: 0159 loss_train: 0.4536 acc_train: 0.9071 loss_val: 0.7273 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0160 loss_train: 0.4354 acc_train: 0.9429 loss_val: 0.7259 acc_val: 0.8133 time: 0.0781s\n",
      "Epoch: 0161 loss_train: 0.4547 acc_train: 0.9429 loss_val: 0.7231 acc_val: 0.8133 time: 0.0625s\n",
      "Epoch: 0162 loss_train: 0.4664 acc_train: 0.9286 loss_val: 0.7197 acc_val: 0.8200 time: 0.0762s\n",
      "Epoch: 0163 loss_train: 0.4322 acc_train: 0.9571 loss_val: 0.7173 acc_val: 0.8233 time: 0.0800s\n",
      "Epoch: 0164 loss_train: 0.4555 acc_train: 0.9500 loss_val: 0.7152 acc_val: 0.8300 time: 0.0760s\n",
      "Epoch: 0165 loss_train: 0.4441 acc_train: 0.9429 loss_val: 0.7126 acc_val: 0.8400 time: 0.0677s\n",
      "Epoch: 0166 loss_train: 0.4468 acc_train: 0.9357 loss_val: 0.7108 acc_val: 0.8400 time: 0.0781s\n",
      "Epoch: 0167 loss_train: 0.4105 acc_train: 0.9357 loss_val: 0.7099 acc_val: 0.8233 time: 0.1024s\n",
      "Epoch: 0168 loss_train: 0.4120 acc_train: 0.9357 loss_val: 0.7091 acc_val: 0.8200 time: 0.0798s\n",
      "Epoch: 0169 loss_train: 0.4517 acc_train: 0.9286 loss_val: 0.7087 acc_val: 0.8200 time: 0.0781s\n",
      "Epoch: 0170 loss_train: 0.4212 acc_train: 0.9429 loss_val: 0.7089 acc_val: 0.8200 time: 0.0625s\n",
      "Epoch: 0171 loss_train: 0.4592 acc_train: 0.9357 loss_val: 0.7100 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0172 loss_train: 0.4400 acc_train: 0.9357 loss_val: 0.7095 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0173 loss_train: 0.4101 acc_train: 0.9500 loss_val: 0.7099 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0174 loss_train: 0.4042 acc_train: 0.9214 loss_val: 0.7083 acc_val: 0.8200 time: 0.0746s\n",
      "Epoch: 0175 loss_train: 0.4001 acc_train: 0.9286 loss_val: 0.7058 acc_val: 0.8100 time: 0.0948s\n",
      "Epoch: 0176 loss_train: 0.4301 acc_train: 0.9571 loss_val: 0.7040 acc_val: 0.8100 time: 0.0781s\n",
      "Epoch: 0177 loss_train: 0.4519 acc_train: 0.9571 loss_val: 0.7014 acc_val: 0.8100 time: 0.0781s\n",
      "Epoch: 0178 loss_train: 0.4183 acc_train: 0.9357 loss_val: 0.7001 acc_val: 0.8133 time: 0.0781s\n",
      "Epoch: 0179 loss_train: 0.4013 acc_train: 0.9429 loss_val: 0.6978 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0180 loss_train: 0.3573 acc_train: 0.9571 loss_val: 0.6953 acc_val: 0.8200 time: 0.0781s\n",
      "Epoch: 0181 loss_train: 0.4100 acc_train: 0.9429 loss_val: 0.6928 acc_val: 0.8300 time: 0.0781s\n",
      "Epoch: 0182 loss_train: 0.4000 acc_train: 0.9500 loss_val: 0.6918 acc_val: 0.8200 time: 0.0781s\n",
      "Epoch: 0183 loss_train: 0.4079 acc_train: 0.9429 loss_val: 0.6922 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0184 loss_train: 0.4035 acc_train: 0.9429 loss_val: 0.6928 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0185 loss_train: 0.3986 acc_train: 0.9571 loss_val: 0.6937 acc_val: 0.8167 time: 0.0781s\n",
      "Epoch: 0186 loss_train: 0.4399 acc_train: 0.9286 loss_val: 0.6931 acc_val: 0.8100 time: 0.0849s\n",
      "Epoch: 0187 loss_train: 0.3946 acc_train: 0.9571 loss_val: 0.6907 acc_val: 0.8067 time: 0.0770s\n",
      "Epoch: 0188 loss_train: 0.4334 acc_train: 0.9143 loss_val: 0.6893 acc_val: 0.8067 time: 0.0780s\n",
      "Epoch: 0189 loss_train: 0.3884 acc_train: 0.9357 loss_val: 0.6872 acc_val: 0.8067 time: 0.0674s\n",
      "Epoch: 0190 loss_train: 0.4208 acc_train: 0.9214 loss_val: 0.6862 acc_val: 0.8100 time: 0.0781s\n",
      "Epoch: 0191 loss_train: 0.4260 acc_train: 0.9429 loss_val: 0.6855 acc_val: 0.8100 time: 0.0781s\n",
      "Epoch: 0192 loss_train: 0.4172 acc_train: 0.9643 loss_val: 0.6845 acc_val: 0.8100 time: 0.0781s\n",
      "Epoch: 0193 loss_train: 0.3733 acc_train: 0.9500 loss_val: 0.6831 acc_val: 0.8133 time: 0.0781s\n",
      "Epoch: 0194 loss_train: 0.3663 acc_train: 0.9571 loss_val: 0.6812 acc_val: 0.8133 time: 0.0781s\n",
      "Epoch: 0195 loss_train: 0.3854 acc_train: 0.9500 loss_val: 0.6796 acc_val: 0.8200 time: 0.0781s\n",
      "Epoch: 0196 loss_train: 0.3440 acc_train: 0.9643 loss_val: 0.6782 acc_val: 0.8200 time: 0.0781s\n",
      "Epoch: 0197 loss_train: 0.4035 acc_train: 0.9500 loss_val: 0.6774 acc_val: 0.8200 time: 0.0781s\n",
      "Epoch: 0198 loss_train: 0.3905 acc_train: 0.9357 loss_val: 0.6782 acc_val: 0.8200 time: 0.0863s\n",
      "Epoch: 0199 loss_train: 0.4133 acc_train: 0.9429 loss_val: 0.6804 acc_val: 0.8100 time: 0.0760s\n",
      "Epoch: 0200 loss_train: 0.4223 acc_train: 0.9000 loss_val: 0.6835 acc_val: 0.8100 time: 0.0780s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 16.4381s\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (1433 -> 16)\n",
      "  (gc2): GraphConvolution (16 -> 7)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "  GraphConvolution-1                   [-1, 16]          22,944\n",
      "  GraphConvolution-2                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 23,063\n",
      "Trainable params: 23,063\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4779.90\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 4779.99\n",
      "----------------------------------------------------------------\n",
      "Test set results: loss= 1.0315 accuracy= 1.0000\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from torchsummary import summary\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "#import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "#from utils import load_data, accuracy\n",
    "#from models import GCN\n",
    "\n",
    "# Training settings\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "\n",
    "#parser.add_argument('--hidden2', type=int, default=8,\n",
    "                    #help='Number of hidden units.')\n",
    "\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "#args = parser.parse_args()\n",
    "#error midad khate bala,khate paein jaigozin shod\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            #nhid2=args.hidden2,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "## to set cuda as your device if possible\n",
    "##training on  GPU\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "    \n",
    "    ## train:adjust the weights on the neural network\n",
    "    ## validation:used to minimize overfitting\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    ## Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "print(model)\n",
    "\n",
    "summary(model,[(2708,1433),(2708,2708)])\n",
    "# Testing\n",
    "test()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    8,   14,  ..., 1389, 2344, 2707],\n",
       "                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),\n",
       "       values=tensor([0.1667, 0.1667, 0.0500,  ..., 0.2000, 0.5000, 0.2500]),\n",
       "       size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor(3),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 2708])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 6,  ..., 4, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213,\n",
       "        214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
       "        228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
       "        242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255,\n",
       "        256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
       "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
       "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
       "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
       "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
       "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
       "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
       "        382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,\n",
       "        396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409,\n",
       "        410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423,\n",
       "        424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437,\n",
       "        438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
       "        452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "        466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479,\n",
       "        480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "        494, 495, 496, 497, 498, 499])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " idx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([500])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
